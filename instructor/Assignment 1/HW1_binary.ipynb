{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2c15dac0-bea3-4ef7-8968-798d69efefd3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"HW1: Implementing logistic regression \"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5de41f-a061-4451-8b50-41aec0f95c7b",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Write your code in the **Code cells** and your answers in the **Markdown cells** of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to render the **.ipynb** file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The assignment is worth 100 points, and is due on **17th October 2025 at 11:59 pm**. \n",
    "\n",
    "5. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "    - Must be an HTML file rendered using Quarto **(1 point)**. *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.* \n",
    "    - No name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission.  **(1 point)**\n",
    "    - There aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) **(1 point)**\n",
    "    - Final answers to each question are written in the Markdown cells. **(1 point)**\n",
    "    - There is no piece of unnecessary / redundant code, and no unnecessary / redundant text. **(1 point)**\n",
    "\n",
    "6.  The maximum possible score in the assigment is 100+5 = 110 out of 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37860adb",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- **Reinforce Knowledge of Gradient Descent:** Apply your understanding of gradient descent to classification problems by implementing logistic regression and softmax regression from scratch.\n",
    "- **Hands-on Implementation:** Build classification models manually to gain deeper insights into their mathematical foundations and working principles.\n",
    "- **Explore Customization Options:** Learn how implementing models from scratch allows you to:\n",
    "  - Adjust and optimize model parameters for specific requirements.\n",
    "  - Add features or constraints that might not be possible with standard libraries.\n",
    "- **Compare with Pre-built Models:** Use scikit-learn’s logistic regression as a baseline to evaluate the performance and efficiency of your custom implementation. This will help you understand when to use custom models and when to leverage pre-built ones.\n",
    "- **Prepare for Real-world Scenarios:** Understand the scenarios where off-the-shelf models are not sufficient, allowing you to confidently tackle complex machine learning problems and create novel solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f39540-d78d-4cec-8ee5-51229dcad5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f9358-3bba-42c7-9b0c-8792a1327918",
   "metadata": {},
   "source": [
    "## Sigmoid or Logistic Function\n",
    "<img align=\"left\" src=\"https://media.licdn.com/dms/image/D4D12AQGIXdSG7IJCNw/article-cover_image-shrink_600_2000/0/1694183259537?e=2147483647&v=beta&t=OtnfeqwCtKTSVrdKZdyOzNYECyLLZuEUIxkTfTQ0dS0\"     style=\" width:300px; padding: 10px; \" >\n",
    "\n",
    "As you learned from the sequence course, for a classification task, we can start by using our linear regression model,\n",
    "\n",
    " $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b$, to predict $y$ given $x$. \n",
    "\n",
    "- However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1. \n",
    "- This can be accomplished by using a \"sigmoid function\" which maps all input values to values between 0 and 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b347e1-8907-490d-bf66-15f8a8c9b568",
   "metadata": {},
   "source": [
    "## Formula for Sigmoid function\n",
    "\n",
    "The formula for a sigmoid function is as follows -  \n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}} \\tag{1}$$\n",
    "\n",
    "\n",
    "In the case of logistic regression, z (the input to the sigmoid function), is the output of a linear regression model. \n",
    "- In the case of a single example, $z$ is scalar.\n",
    "- in the case of multiple examples, $z$ may be a vector consisting of $m$ values, one for each example.\n",
    "- NumPy has a function called [`exp()`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html), which offers a convenient way to calculate the exponential ( $e^{z}$) of all elements in the input array (`z`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c69ac8-0c00-4827-8921-b6550ed8d5c2",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticRegression_right.png\"     style=\" width:300px; padding: 10px; \" > A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:\n",
    "\n",
    "$$ \n",
    "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{2} \n",
    "$$ \n",
    "\n",
    "  where\n",
    "\n",
    "  $$\n",
    "  g(z) = \\frac{1}{1+e^{-z}}\\tag{3}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5760f-12e3-4864-bf3b-cb42ad2f3ca3",
   "metadata": {},
   "source": [
    "## Logistic Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cb12b-65aa-4c91-a642-79684d0b58c5",
   "metadata": {},
   "source": [
    "Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. \n",
    "\n",
    ">**Definition Note:**   In this course, these definitions are used:  \n",
    "**Loss** is a measure of the difference of a single example to its target value while the  \n",
    "**Cost** is a measure of the losses over the training set\n",
    "\n",
    "\n",
    "This is defined: \n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTIyEcDUIGx9kLDohkOyjq8X2OkQZbxLoVW3JyEefVtog&s alt=\"Description of image\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59402019-5be0-4b2f-854a-1eb20e79e6d0",
   "metadata": {},
   "source": [
    "Combined, the curves are similar to the quadratic curve of the squared error loss. Note, the x-axis is $f_{\\mathbf{w},b}$ which is the output of a sigmoid. The sigmoid output is strictly between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba0f78-881b-4dfd-a2aa-e1ac1aa6b061",
   "metadata": {},
   "source": [
    "The loss function above can be rewritten to be easier to implement.\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "This is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  \n",
    "when $ y^{(i)} = 0$, the left-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "and when $ y^{(i)} = 1$, the right-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let's take a look at the cost vs parameters curve for the simple example we considered above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7e36d-723d-4cdb-a772-412ff188130e",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "Recall, loss is defined to apply to one example. Here you combine the losses to form the **cost**, which includes all the examples.\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "*  where m is the number of training examples in the data set and:\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70e382-25b8-470c-8aea-c4fbb10856c4",
   "metadata": {},
   "source": [
    "## Task 1: Implementing L2 Logistic Regression with Vectorized Gradient Descent (10 points)\n",
    "- It should take eight inputs: **X_train**, **y_train**, **X_test**, **y_test**, **w_in**, **alpha** (learning rate), **num_iters**, and **lambda_reg**. \n",
    "- It should return the optimal parameters and the costs history on both train and test set\n",
    "\n",
    "hints:\n",
    "* Implement `compute_cost_logistic_ridge` function to calculate the cost .\n",
    "* Derive gradient for L2 logistic regression\n",
    "* Implement `compute_gradient_logistic_ridge` function to Calculate the Gradient\n",
    "* Implement `gradient_descent_logistic_ridge` function\n",
    "\n",
    "Note that you don't have to stricitly follow these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788436e-7f8f-46c4-9d0d-d1f629921869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa3f1d8-5663-4115-aa84-a241a059611e",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2: Apply your implementation on a data set (10 points)\n",
    "\n",
    "*  Read the **heart_disease_classification.csv** file into pandas dataframe. use **random_state=0** to Shuffle the data to eliminate any inherent order or bias that may be present.\n",
    "*  Split the features and the target column into different variables. **(3 points)**\n",
    "*  Create binary columns from these three categorical columns `cp`, `thal`, and `slope`. **(4 points)**\n",
    "*  Use **random_state=42** to Split the data into training and test datasets with a 80-20 split. . Then, scale the features of both datasets. **(4 points)**\n",
    "*  Set initial w_in, alpha, num_iters, and lambda_reg you think are right, as long as the model converges.\n",
    "*  Plot the learning curve of gradient descent on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c82e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26bf8091-bab2-4738-aad7-589594f261a9",
   "metadata": {},
   "source": [
    "## Task 3: Explore the impact of `lambda_reg` on the dataset and identify the optimal value that provides the best performance (10 points)\n",
    "\n",
    "`lambda_reg` controls the strength of the regularization applied to the model. When `lambda_reg` is set to zero, regularization is effectively turned off. As `lambda_reg` increases, the penalty for large weights becomes more significant, helping to reduce overfitting. In this task, \n",
    "\n",
    "* Experiment with different values of `lambda_reg` in the set [0.0, 0.01, 0.03, 0.1, 0.3]. \n",
    "* Plot the learning curves for both the training and test sets on the same figure to visualize the impact of each value.\n",
    "* Determine which value of `lambda_reg` yields the best performance on this dataset.\n",
    "* Output the performance in terms of `accuracy`, `precision`, `recall`\n",
    "\n",
    "Please use `learning_rate=0.005, num_iterations=1200` for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1207a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5f362b-2348-4a83-9b6c-545a14ab7a3a",
   "metadata": {},
   "source": [
    "## Task 4: Compare your implementation with the `LogisticRegression` model from `sklearn` to re-evaluate the dataset (10 points)\n",
    "\n",
    "* Use the `LogisticRegression` model from `sklearn` to re-evaluate the dataset while maintaining the  maintaining the same (or similar) hyperparameter settings for a (fair) comparison. \n",
    "* Report the performance using the same evaluation metrics as previously used, and \n",
    "* Compare the results to your custom implementation. Analyze whether `sklearn`'s built-in logistic regression achieves similar, better, or worse performance, and \n",
    "* Try to explain the potential reasons for any differences observed.  (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f11d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7f2c627-aaa2-4dc0-a68c-dd4dea175ae5",
   "metadata": {},
   "source": [
    "## Task 5: Get to know the `tol` in Sklearn `LogisticRegression` (10 points)\n",
    "\n",
    "Use data visualization to explore the impact of the `tol` (tolerance) parameter in the logistic regression model. Explain how it affects model's overall performance.\n",
    "\n",
    "Hints: Feel free to experiment with different values to see how this hyperparameter affects the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93239c-180f-49fa-b77d-b1ece150fdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5baf1dfd",
   "metadata": {},
   "source": [
    "## Task 6: Summarize your findings below (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6e28f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
